{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanskarJadhav/NaturalLanguageProcessing/blob/main/NLP_IMDBSentiment_6076.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-Kbhpnsb8VN"
      },
      "source": [
        "# Sentiment Analysis Performed On IMDB Dataset\n",
        "By Sanskar Jadhav PRN 21070126076 AIML B1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt9fvisVZFqK",
        "outputId": "1bd22c4e-1ff8-446c-a82e-8e94e21fe686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# importing the required libraries for preprocessing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "szxU6c7Yfm8l",
        "outputId": "f3d783ed-651d-4356-b8ce-4d18b432c174"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-7bc5eeaf-f9a3-4300-ac3a-702d1009f0b7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7bc5eeaf-f9a3-4300-ac3a-702d1009f0b7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-16634a0c-f056-403d-a6ea-e16536189a74\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-16634a0c-f056-403d-a6ea-e16536189a74')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-16634a0c-f056-403d-a6ea-e16536189a74 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7bc5eeaf-f9a3-4300-ac3a-702d1009f0b7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7bc5eeaf-f9a3-4300-ac3a-702d1009f0b7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# saving the dataset in a dataframe\n",
        "# i'm taking the dataset from my dropbox, so no need to upload to Colab manually\n",
        "imdb_df = pd.read_csv(\"https://www.dropbox.com/scl/fi/by1d23shj320pz7wcljb6/IMDB-Dataset.csv?rlkey=448p28vifmtrx38bf24b16ypy&dl=1\")\n",
        "imdb_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGfdalZqgTuR",
        "outputId": "030f3e87-a13d-41c6-e962-254829bf4a79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     50000 non-null  object\n",
            " 1   sentiment  50000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.4+ KB\n"
          ]
        }
      ],
      "source": [
        "# check for null values\n",
        "imdb_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5fMvln4ft7_",
        "outputId": "6f3b918a-b95d-4c09-e4da-118beaf124d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    One of the other reviewers has mentioned that ...\n",
              "1    A wonderful little production. <br /><br />The...\n",
              "2    I thought this was a wonderful way to spend ti...\n",
              "3    Basically there's a family where a little boy ...\n",
              "4    Petter Mattei's \"Love in the Time of Money\" is...\n",
              "Name: review, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# selecting the reviews column for text preprocessing\n",
        "reviews = imdb_df['review']\n",
        "reviews[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpGPKChWn36k"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xqk2hL3Vge0H"
      },
      "outputs": [],
      "source": [
        "# note that there are some additional cleaning steps required\n",
        "clean_reviews = []\n",
        "# defining a regular expression pattern to match URLs\n",
        "url_pattern = re.compile(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", re.MULTILINE)\n",
        "for review in reviews:\n",
        "  # handling html tags before removing symbols\n",
        "  review = re.sub(\"\\n\", \" \", review)\n",
        "  review = re.sub(\"<br /><br />\", \" \", review)\n",
        "  # removing URLs\n",
        "  review = re.sub(url_pattern, \" \", review)\n",
        "  # removing years\n",
        "  review = re.sub(\"\\d{4}\", \" \", review)\n",
        "  # removing symbols\n",
        "  review = re.sub(\"-\", \" \", review)\n",
        "  review = re.sub(\"'\", \"\", review)\n",
        "  review = re.sub(\"[^A-Za-z0-9 ]+\",\" \", review)\n",
        "  # removing extra spaces\n",
        "  review = re.sub(\" +\", \" \", review)\n",
        "  clean_reviews.append(review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anw8sdkXhLfG",
        "outputId": "c42a4194-26bd-4f88-88fc-58f238cebc35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['One of the other reviewers has mentioned that after watching just 1 Oz episode youll be hooked They are right as this is exactly what happened with me The first thing that struck me about Oz was its brutality and unflinching scenes of violence which set in right from the word GO Trust me this is not a show for the faint hearted or timid This show pulls no punches with regards to drugs sex or violence Its is hardcore in the classic use of the word It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary It focuses mainly on Emerald City an experimental section of the prison where all the cells have glass fronts and face inwards so privacy is not high on the agenda Em City is home to many Aryans Muslims gangstas Latinos Christians Italians Irish and more so scuffles death stares dodgy dealings and shady agreements are never far away I would say the main appeal of the show is due to the fact that it goes where other shows wouldnt dare Forget pretty pictures painted for mainstream audiences forget charm forget romance OZ doesnt mess around The first episode I ever saw struck me as so nasty it was surreal I couldnt say I was ready for it but as I watched more I developed a taste for Oz and got accustomed to the high levels of graphic violence Not just violence but injustice crooked guards wholl be sold out for a nickel inmates wholl kill on order and get away with it well mannered middle class inmates being turned into prison bitches due to their lack of street skills or prison experience Watching Oz you may become comfortable with what is uncomfortable viewing thats if you can get in touch with your darker side ',\n",
              " 'A wonderful little production The filming technique is very unassuming very old time BBC fashion and gives a comforting and sometimes discomforting sense of realism to the entire piece The actors are extremely well chosen Michael Sheen not only has got all the polari but he has all the voices down pat too You can truly see the seamless editing guided by the references to Williams diary entries not only is it well worth the watching but it is a terrificly written and performed piece A masterful production about one of the great masters of comedy and his life The realism really comes home with the little things the fantasy of the guard which rather than use the traditional dream techniques remains solid then disappears It plays on our knowledge and our senses particularly with the scenes concerning Orton and Halliwell and the sets particularly of their flat with Halliwells murals decorating every surface are terribly well done ',\n",
              " 'I thought this was a wonderful way to spend time on a too hot summer weekend sitting in the air conditioned theater and watching a light hearted comedy The plot is simplistic but the dialogue is witty and the characters are likable even the well bread suspected serial killer While some may be disappointed when they realize this is not Match Point 2 Risk Addiction I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love This was the most Id laughed at one of Woodys comedies in years dare I say a decade While Ive never been impressed with Scarlet Johanson in this she managed to tone down her sexy image and jumped right into a average but spirited young woman This may not be the crown jewel of his career but it was wittier than Devil Wears Prada and more interesting than Superman a great comedy to go see with friends ',\n",
              " 'Basically theres a family where a little boy Jake thinks theres a zombie in his closet his parents are fighting all the time This movie is slower than a soap opera and suddenly Jake decides to become Rambo and kill the zombie OK first of all when youre going to make a film you must Decide if its a thriller or a drama As a drama the movie is watchable Parents are divorcing arguing like in real life And then we have Jake with his closet which totally ruins all the film I expected to see a BOOGEYMAN similar movie and instead i watched a drama with some meaningless thriller spots 3 out of 10 just for the well playing parents descent dialogs As for the shots with Jake just ignore them ',\n",
              " 'Petter Matteis Love in the Time of Money is a visually stunning film to watch Mr Mattei offers us a vivid portrait about human relations This is a movie that seems to be telling us what money power and success do to people in the different situations we encounter This being a variation on the Arthur Schnitzlers play about the same theme the director transfers the action to the present time New York where all these different characters meet and connect Each one is connected in one way or another to the next person but no one seems to know the previous point of contact Stylishly the film has a sophisticated luxurious look We are taken to see how these people live and the world they live in their own habitat The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits A big city is not exactly the best place in which human relations find sincere fulfillment as one discerns is the case with most of the people we encounter The acting is good under Mr Matteis direction Steve Buscemi Rosario Dawson Carol Kane Michael Imperioli Adrian Grenier and the rest of the talented cast make these characters come alive We wish Mr Mattei good luck and await anxiously for his next work ']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "clean_reviews[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "0XDci3u3h1Pa",
        "outputId": "ad505c4d-5bdf-4e0e-e076-29629ddea72c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eir lack of street skills or prison experience Watching Oz you may become comfortable with what is uncomfortable viewing thats if you can get in touch with your darker side A wonderful little production The filming technique is very unassuming very old time BBC fashion and gives a comforting and sometimes discomforting sense of realism to the entire piece The actors are extremely well chosen Michael Sheen not only has got all the polari but he has all the voices down pat too You can truly see the seamless editing guided by the references to Williams diary entries not only is it well worth the watching but it is a terrificly written and performed piece A masterful production about one of the great masters of comedy and his life The realism really comes home with the little things the fantasy of the guard which rather than use the traditional dream techniques remains solid then disappears It plays on our knowledge and our senses particularly with the scenes concerning Orton and Halliwell and the sets particularly of their flat with Halliwells murals decorating every surface are terribly well done I thought this was a wonderful way to spend time on a too hot summer weekend sitting in the air conditioned theater and watching a light hearted comedy The plot is simplistic but the dialogue is witty and the characters are likable even the well bread suspected serial killer While some may be disappointed when they realize this is not Match Point 2 Risk Addiction I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love This was the most Id laughed at one of Woodys comedies in years dare I say a decade While Ive never been impressed with Scarlet Johanson in this she managed to tone down her sexy image and jumped right into a average but spirited young woman This may not be the crown jewel of his career but it was wittier than Devil Wears Prada and more interesting than Superman a great comedy to go see with friends Basically theres a family where a little boy Jake thinks theres a zombie in his closet his parents are fightin'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "full_text = \"\".join([review for review in clean_reviews])\n",
        "full_text[1500:3600] # to confirm it's one large string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Cy5KE_lmYnc",
        "outputId": "5f9503d6-2034-4ce5-c68b-8b1ad83114fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11536215\n"
          ]
        }
      ],
      "source": [
        "# make all text lowercase\n",
        "full_text = full_text.lower()\n",
        "# tokenization\n",
        "tokens = word_tokenize(full_text)\n",
        "print(len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMt-fuoEogNZ"
      },
      "outputs": [],
      "source": [
        "stop_words = []\n",
        "new_stopwords = [\"you'll\", \"i've\", \"i'll\", \"who'll\", \"they'll\", \"we've\", \"a\",\n",
        "                 \"i\", \"i'd\", \"i'm\", \"my\", \"his\", \"her\", \"our\", \"their\"]\n",
        "swlist = stopwords.words('english')\n",
        "swlist.extend(new_stopwords)\n",
        "for word in swlist:\n",
        "  word = re.sub(\"'\", \"\", word)\n",
        "  stop_words.append(word)\n",
        "set_stop_words = set(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16IAkSt5vL1J",
        "outputId": "e7572d54-d0ad-4b94-e5e8-9eaffda09237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'himself', 'did', 'of', 'hasn', 'they', 'here', 'aren', 'im', 'not', 'up', 'm', 'because', 'you', 'can', 'o', 'was', 'about', 'we', 't', 'yourselves', 'shouldnt', 'just', 'that', 'a', 'didn', 'hadnt', 'above', 'is', 'couldn', 'against', 'into', 'wasn', 'ill', 'its', 'hasnt', 'under', 'same', 'before', 'weren', 'when', 'themselves', 'myself', 'whom', 'after', 'these', 'do', 'youre', 'than', 'i', 'ive', 'will', 'hers', 'doesnt', 'ain', 'by', 'll', 'isn', 'if', 'for', 'shouldve', 'needn', 'am', 'wouldnt', 'have', 'don', 'won', 'havent', 'what', 'been', 'itself', 'has', 'down', 'further', 'd', 'mustnt', 'shant', 'shes', 'does', 'so', 'from', 'mightnt', 'as', 'isnt', 'shan', 'off', 'weve', 'mustn', 'his', 'should', 'again', 've', 'until', 'wont', 'youve', 'shouldn', 'haven', 'then', 'very', 'nor', 'them', 'were', 'id', 'now', 'mightn', 'on', 'once', 'ma', 'which', 'at', 'it', 're', 'with', 'wholl', 'but', 'couldnt', 'or', 'how', 'only', 'more', 'over', 'theirs', 'who', 'most', 'those', 'being', 'some', 'own', 'youd', 'dont', 'yours', 'both', 'thatll', 'doing', 'each', 'your', 'herself', 'no', 'in', 'their', 'youll', 'wouldn', 'our', 'through', 'during', 'any', 'y', 'too', 'had', 'why', 'me', 'yourself', 'he', 'such', 'theyll', 'an', 'wasnt', 'below', 'didnt', 'doesn', 'ourselves', 'are', 'there', 'hadn', 'out', 'my', 'other', 'this', 'where', 'all', 'between', 'to', 'neednt', 'having', 'the', 'and', 'arent', 'her', 'few', 'she', 'him', 's', 'ours', 'werent', 'while', 'be'}\n"
          ]
        }
      ],
      "source": [
        "print(set_stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny0Ro6yVvZxR",
        "outputId": "163d8033-1a39-4386-ad1f-73f34553e185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5995506\n"
          ]
        }
      ],
      "source": [
        "filtered_tokens = [w for w in tokens if not w in set_stop_words]\n",
        "print(len(filtered_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbeK1rTRxdjC",
        "outputId": "15a7c2ed-78ca-47b9-d05d-57b61c245f62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "109636\n"
          ]
        }
      ],
      "source": [
        "print(len(set(filtered_tokens)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlKqq8Ljw4Pa",
        "outputId": "1ba10e64-6f56-4f1b-8aaa-bf5508f15463"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['one reviewers mentioned watching 1 oz episode hooked right exactly happened first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use word called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home many aryans muslims gangstas latinos christians italians irish scuffles death stares dodgy dealings shady agreements never far away would say main appeal show due fact goes shows dare forget pretty pictures painted mainstream audiences forget charm forget romance oz mess around first episode ever saw struck nasty surreal say ready watched developed taste oz got accustomed high levels graphic violence violence injustice crooked guards sold nickel inmates kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewing thats get touch darker side',\n",
              " 'wonderful little production filming technique unassuming old time bbc fashion gives comforting sometimes discomforting sense realism entire piece actors extremely well chosen michael sheen got polari voices pat truly see seamless editing guided references williams diary entries well worth watching terrificly written performed piece masterful production one great masters comedy life realism really comes home little things fantasy guard rather use traditional dream techniques remains solid disappears plays knowledge senses particularly scenes concerning orton halliwell sets particularly flat halliwells murals decorating every surface terribly well done',\n",
              " 'thought wonderful way spend time hot summer weekend sitting air conditioned theater watching light hearted comedy plot simplistic dialogue witty characters likable even well bread suspected serial killer may disappointed realize match point 2 risk addiction thought proof woody allen still fully control style many us grown love laughed one woodys comedies years dare say decade never impressed scarlet johanson managed tone sexy image jumped right average spirited young woman may crown jewel career wittier devil wears prada interesting superman great comedy go see friends',\n",
              " 'basically theres family little boy jake thinks theres zombie closet parents fighting time movie slower soap opera suddenly jake decides become rambo kill zombie ok first going make film must decide thriller drama drama movie watchable parents divorcing arguing like real life jake closet totally ruins film expected see boogeyman similar movie instead watched drama meaningless thriller spots 3 10 well playing parents descent dialogs shots jake ignore',\n",
              " 'petter matteis love time money visually stunning film watch mr mattei offers us vivid portrait human relations movie seems telling us money power success people different situations encounter variation arthur schnitzlers play theme director transfers action present time new york different characters meet connect one connected one way another next person one seems know previous point contact stylishly film sophisticated luxurious look taken see people live world live habitat thing one gets souls picture different stages loneliness one inhabits big city exactly best place human relations find sincere fulfillment one discerns case people encounter acting good mr matteis direction steve buscemi rosario dawson carol kane michael imperioli adrian grenier rest talented cast make characters come alive wish mr mattei good luck await anxiously next work']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Thus, we have reduced the number of tokens from over 11 million to nearly 100,000!\n",
        "tokens_in_reviews = []\n",
        "filteredtokenset = set(filtered_tokens)\n",
        "for i in clean_reviews:\n",
        "  review = i.lower()\n",
        "  words = review.split(\" \")\n",
        "  listoftokens = [token for token in words if token in filteredtokenset]\n",
        "  stringoftokens = \" \".join(listoftokens)\n",
        "  tokens_in_reviews.append(stringoftokens)\n",
        "tokens_in_reviews[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTpIjG5w-1IK"
      },
      "outputs": [],
      "source": [
        "# perform lemmatization with SpaCy\n",
        "lemmatizedreviews = []\n",
        "for doc in nlp.pipe(tokens_in_reviews, batch_size=500, n_process=2,\n",
        "                    disable=[\"parser\", \"ner\"]):\n",
        "    lemstring = \" \".join([tok.lemma_ for tok in doc])\n",
        "    #print(lemstring) did this to confirm it was running correctly\n",
        "    lemmatizedreviews.append(lemstring)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaG41tdrnyUX"
      },
      "source": [
        "## Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "HMOApA85OEEH",
        "outputId": "28b83845-1f2a-43d9-d545-2707b055bd34"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "countvector = CountVectorizer()\n",
        "countvector.fit(lemmatizedreviews)\n",
        "#print(\"Vocabulary: \", countvector.vocabulary_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39oaXS31QC7g",
        "outputId": "642ff3a1-79a0-415e-d288-7448c21df2ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 91689)\n"
          ]
        }
      ],
      "source": [
        "# encode document\n",
        "cv_reviews = countvector.transform(lemmatizedreviews)\n",
        "# summarize encoded vector\n",
        "print(cv_reviews.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "xplvYB-sdYBU",
        "outputId": "a964736a-edd6-4a2b-facb-86d1eac60c39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidfvector = TfidfVectorizer()\n",
        "tfidfvector.fit(lemmatizedreviews)\n",
        "#print(\"Vocabulary: \", tfidfvector.vocabulary_)\n",
        "# printing vocabulary of count and tfidf took up >900 pages in the PDF\n",
        "# thus commented the code and ran again before downloading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIYtqXQHQT99",
        "outputId": "8c002096-b005-4841-ebf4-075a8a6f0d2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 91689)\n"
          ]
        }
      ],
      "source": [
        "tv_reviews = tfidfvector.transform(lemmatizedreviews)\n",
        "print(tv_reviews.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjvHGobqedAg"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "# Word2Vec model using CBOW architecture\n",
        "lemwordlist = []\n",
        "for i in lemmatizedreviews:\n",
        "  words = i.split(\" \")\n",
        "  lemwordlist.append(words)\n",
        "model1 = gensim.models.Word2Vec(lemwordlist, sg=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUu-PHX6ks-1",
        "outputId": "fb0e8ed7-afb8-4b11-a203-465377fb2415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32550\n"
          ]
        }
      ],
      "source": [
        "print(len(model1.wv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3gZ_IDhcV7K"
      },
      "outputs": [],
      "source": [
        "w2v_vectors = []\n",
        "for review in lemwordlist:\n",
        "    vectors = [model1.wv[word] for word in review if word in model1.wv]\n",
        "    if vectors: # some reviews may have no words in model's vocab\n",
        "      doc_vector = np.mean(vectors, axis=0)\n",
        "      w2v_vectors.append(doc_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fTrCAuPlmN1",
        "outputId": "b4f21f1d-623d-43af-8c5b-202b4b1511dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[=================================================-] 100.0% 1662.6/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "# GoogleNews Word2Vec\n",
        "import gensim.downloader as api\n",
        "model2 = api.load(\"word2vec-google-news-300\")  # download the model and return as object ready for use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OPhDPR1mha_"
      },
      "outputs": [],
      "source": [
        "googlew2v_vectors = []\n",
        "for review in lemwordlist:\n",
        "    vectors = [model2[word] for word in review if word in model2]\n",
        "    if vectors: # some reviews may have no words in model's vocab\n",
        "      doc_vector = np.mean(vectors, axis=0)\n",
        "      googlew2v_vectors.append(doc_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0gjjnw-nqDs"
      },
      "source": [
        "## Machine Learning Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtV7QvTBq_Qx"
      },
      "source": [
        "According to the Kaggle website, the expected train:test ratio to be used is 50:50. Also, keeping shuffle = False so that the testing set is the same for all algorithms for a fair evaluation and comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYlDLAJToGO7"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFMvnu7yMci-",
        "outputId": "32c9cb1c-10b7-4504-d46a-142223232bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Accuracy: 0.884\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.89      0.88      0.88     12474\n",
            "    positive       0.88      0.89      0.88     12526\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10949  1525]\n",
            " [ 1375 11151]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "# using cv_reviews\n",
        "X_train, X_test, y_train, y_test = train_test_split(cv_reviews, imdb_df['sentiment'], test_size=0.5, shuffle=False)\n",
        "# defining the hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10],  # regularization parameters\n",
        "    'max_iter': [300],     # limit max no. of iterations to a balance between time and convergence\n",
        "}\n",
        "logreg = LogisticRegression()\n",
        "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
        "# finding the best parameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_model = grid_search.best_estimator_\n",
        "# making predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Best Model Accuracy:\", accuracy)\n",
        "# classification report\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", cr)\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iARYcTgBrNS0"
      },
      "outputs": [],
      "source": [
        "logresacc = []\n",
        "logresacc.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdoxLlpt1zLC",
        "outputId": "4b2e5cc5-3647-476c-9d14-8e80d2aa7b3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Best Model Accuracy: 0.88916\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.90      0.88      0.89     12474\n",
            "    positive       0.88      0.90      0.89     12526\n",
            "\n",
            "    accuracy                           0.89     25000\n",
            "   macro avg       0.89      0.89      0.89     25000\n",
            "weighted avg       0.89      0.89      0.89     25000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10985  1489]\n",
            " [ 1282 11244]]\n"
          ]
        }
      ],
      "source": [
        "# using tv_reviews\n",
        "X_train, X_test, y_train, y_test = train_test_split(tv_reviews, imdb_df['sentiment'], test_size=0.5, shuffle=False)\n",
        "# defining the hyperparameter grid for GridSearchCV\n",
        "logreg = LogisticRegression()\n",
        "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
        "# finding the best parameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_model = grid_search.best_estimator_\n",
        "# making predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "logresacc.append(accuracy)\n",
        "print(\"Best Model Accuracy:\", accuracy)\n",
        "# classification report\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", cr)\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78z3jNkA2EvE",
        "outputId": "4452dde3-2919-4afb-906a-b461ecf78547"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Best Model Accuracy: 0.86016\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.86      0.85      0.86     12474\n",
            "    positive       0.86      0.87      0.86     12526\n",
            "\n",
            "    accuracy                           0.86     25000\n",
            "   macro avg       0.86      0.86      0.86     25000\n",
            "weighted avg       0.86      0.86      0.86     25000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10643  1831]\n",
            " [ 1665 10861]]\n"
          ]
        }
      ],
      "source": [
        "# using w2v_vectors\n",
        "X = np.array(w2v_vectors)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, imdb_df['sentiment'], test_size=0.5, shuffle=False)\n",
        "# defining the hyperparameter grid for GridSearchCV\n",
        "logreg = LogisticRegression()\n",
        "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
        "# finding the best parameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_model = grid_search.best_estimator_\n",
        "# making predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "logresacc.append(accuracy)\n",
        "print(\"Best Model Accuracy:\", accuracy)\n",
        "# classification report\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", cr)\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T9oHNRP3k2y",
        "outputId": "eb1f4457-e8f0-459c-9593-905c6d53af2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Best Model Accuracy: 0.85316\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.86      0.85      0.85     12474\n",
            "    positive       0.85      0.86      0.85     12526\n",
            "\n",
            "    accuracy                           0.85     25000\n",
            "   macro avg       0.85      0.85      0.85     25000\n",
            "weighted avg       0.85      0.85      0.85     25000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10570  1904]\n",
            " [ 1767 10759]]\n"
          ]
        }
      ],
      "source": [
        "# using googlew2v_vectors\n",
        "X = np.array(googlew2v_vectors)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, imdb_df['sentiment'], test_size=0.5, shuffle=False)\n",
        "# defining the hyperparameter grid for GridSearchCV\n",
        "logreg = LogisticRegression()\n",
        "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
        "# finding the best parameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_model = grid_search.best_estimator_\n",
        "# making predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "logresacc.append(accuracy)\n",
        "print(\"Best Model Accuracy:\", accuracy)\n",
        "# classification report\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", cr)\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5unBCuD56YO",
        "outputId": "60b020fc-d3d7-4046-ed6f-10791609f3bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.884, 0.88916, 0.86016, 0.85316]\n"
          ]
        }
      ],
      "source": [
        "print(logresacc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbvfJenh6Kiy"
      },
      "source": [
        "Thus, we can see that the best performer in Logistic Regression was TfidfVectorizer, followed closely by CountVectorizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XmbR8IV6UNm"
      },
      "source": [
        "### SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4n8o_417m2J",
        "outputId": "7a12ef94-4122-4770-e7a6-1f39b9b67b09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Accuracy: 0.87276\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.87      0.87      0.87     12474\n",
            "    positive       0.87      0.87      0.87     12526\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10860  1614]\n",
            " [ 1567 10959]]\n"
          ]
        }
      ],
      "source": [
        "svcacc = []\n",
        "from sklearn.svm import SVC\n",
        "# using cv_reviews\n",
        "X_train, X_test, y_train, y_test = train_test_split(cv_reviews, imdb_df['sentiment'], test_size=0.5, shuffle=False)\n",
        "# defining the SVC classifier with your chosen parameters\n",
        "svc_classifier = SVC(C=0.1, kernel='linear')\n",
        "svc_classifier.fit(X_train, y_train)\n",
        "# making predictions\n",
        "y_pred = svc_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "svcacc.append(accuracy)\n",
        "print(\"Best Model Accuracy:\", accuracy)\n",
        "# classification report\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", cr)\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKIA-HCE6WFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e53902-b1c6-461a-b2d0-d719a9616988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Accuracy: 0.87188\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.90      0.83      0.87     12474\n",
            "    positive       0.85      0.91      0.88     12526\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10402  2072]\n",
            " [ 1131 11395]]\n"
          ]
        }
      ],
      "source": [
        "# the previous code took over 20 mins!\n",
        "# using tv_reviews\n",
        "X_train, X_test, y_train, y_test = train_test_split(tv_reviews, imdb_df['sentiment'], test_size=0.5, shuffle=False)\n",
        "# defining the SVC classifier with your chosen parameters\n",
        "svc_classifier = SVC(C=0.1, kernel='linear')\n",
        "svc_classifier.fit(X_train, y_train)\n",
        "# making predictions\n",
        "y_pred = svc_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "svcacc.append(accuracy)\n",
        "print(\"Best Model Accuracy:\", accuracy)\n",
        "# classification report\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", cr)\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using w2v_vectors\n",
        "X = np.array(w2v_vectors)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, imdb_df['sentiment'], test_size=0.5, shuffle=False)\n",
        "# defining the SVC classifier with your chosen parameters\n",
        "svc_classifier = SVC(C=0.1, kernel='linear')\n",
        "svc_classifier.fit(X_train, y_train)\n",
        "# making predictions\n",
        "y_pred = svc_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "svcacc.append(accuracy)\n",
        "print(\"Best Model Accuracy:\", accuracy)\n",
        "# classification report\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", cr)\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT_sdVk0D3JG",
        "outputId": "ccbc9fa4-4cab-4da8-9ffc-c3a203544f98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Accuracy: 0.8614\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.87      0.85      0.86     12474\n",
            "    positive       0.86      0.87      0.86     12526\n",
            "\n",
            "    accuracy                           0.86     25000\n",
            "   macro avg       0.86      0.86      0.86     25000\n",
            "weighted avg       0.86      0.86      0.86     25000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10631  1843]\n",
            " [ 1622 10904]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using googlew2v_vectors\n",
        "X = np.array(googlew2v_vectors)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, imdb_df['sentiment'], test_size=0.5, shuffle=False)\n",
        "# defining the SVC classifier with your chosen parameters\n",
        "svc_classifier = SVC(C=0.1, kernel='linear')\n",
        "svc_classifier.fit(X_train, y_train)\n",
        "# making predictions\n",
        "y_pred = svc_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "svcacc.append(accuracy)\n",
        "print(\"Best Model Accuracy:\", accuracy)\n",
        "# classification report\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", cr)\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaNpyfHcHgIk",
        "outputId": "c5b593a9-fc90-4316-8051-c5fa93a4ad89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Accuracy: 0.83824\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.84      0.83      0.84     12474\n",
            "    positive       0.83      0.84      0.84     12526\n",
            "\n",
            "    accuracy                           0.84     25000\n",
            "   macro avg       0.84      0.84      0.84     25000\n",
            "weighted avg       0.84      0.84      0.84     25000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10379  2095]\n",
            " [ 1949 10577]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(svcacc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uajq5RmJJYgg",
        "outputId": "47c912f5-9b4a-4de9-9f6f-12da7ffb5730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.87276, 0.87188, 0.8614, 0.83824]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we see that SVC performed best with CountVectorizer and then TfidfVectorizer"
      ],
      "metadata": {
        "id": "4BGnJIrwJYHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "lVjtueNNJMfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfacc = []\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# using cv_reviews\n",
        "X_train, X_test, y_train, y_test = train_test_split(cv_reviews, imdb_df['sentiment'], test_size=0.5, shuffle=False)\n",
        "# defining RF classifier with chosen parameters\n",
        "# no need for grid search, we know more trees and more depth will give more accuracy\n",
        "# but also more training time and possible overfitting, so keep balanced values\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,        # no of trees\n",
        "    max_depth=50,            # max depth of tree\n",
        "    min_samples_split=4,     # min samples required for split\n",
        "    min_samples_leaf=2,      # min samples required for leaf\n",
        "    random_state=0\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "# making predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "rfacc.append(accuracy)\n",
        "print(\"Best Model Accuracy:\", accuracy)\n",
        "# classification report\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", cr)\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXJfhYx0IBt0",
        "outputId": "0df5450f-04f7-483c-99b7-96b63b4c19f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Accuracy: 0.85076\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.86      0.84      0.85     12474\n",
            "    positive       0.84      0.86      0.85     12526\n",
            "\n",
            "    accuracy                           0.85     25000\n",
            "   macro avg       0.85      0.85      0.85     25000\n",
            "weighted avg       0.85      0.85      0.85     25000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10458  2016]\n",
            " [ 1715 10811]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using tv_reviews\n",
        "X_train, X_test, y_train, y_test = train_test_split(tv_reviews, imdb_df['sentiment'], test_size=0.5, shuffle=False)\n",
        "# defining RF classifier with chosen parameters\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,        # no of trees\n",
        "    max_depth=50,            # max depth of tree\n",
        "    min_samples_split=4,     # min samples required for split\n",
        "    min_samples_leaf=2,      # min samples required for leaf\n",
        "    random_state=0\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "# making predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "rfacc.append(accuracy)\n",
        "print(\"Best Model Accuracy:\", accuracy)\n",
        "# classification report\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", cr)\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdZ4ywTmK7dd",
        "outputId": "8459debd-2819-4e25-ac57-1d0ed927bbf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Accuracy: 0.84772\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.86      0.83      0.84     12474\n",
            "    positive       0.84      0.87      0.85     12526\n",
            "\n",
            "    accuracy                           0.85     25000\n",
            "   macro avg       0.85      0.85      0.85     25000\n",
            "weighted avg       0.85      0.85      0.85     25000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10336  2138]\n",
            " [ 1669 10857]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using w2v_vectors\n",
        "X = np.array(w2v_vectors)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, imdb_df['sentiment'], test_size=0.5, shuffle=False)\n",
        "# defining RF classifier with chosen parameters\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,        # no of trees\n",
        "    max_depth=50,            # max depth of tree\n",
        "    min_samples_split=4,     # min samples required for split\n",
        "    min_samples_leaf=2,      # min samples required for leaf\n",
        "    random_state=0\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "# making predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "rfacc.append(accuracy)\n",
        "print(\"Best Model Accuracy:\", accuracy)\n",
        "# classification report\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", cr)\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBvNhY-8L8xA",
        "outputId": "1323ef66-cc4d-46f5-de94-d528a35743b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Accuracy: 0.8314\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.84      0.81      0.83     12474\n",
            "    positive       0.82      0.85      0.83     12526\n",
            "\n",
            "    accuracy                           0.83     25000\n",
            "   macro avg       0.83      0.83      0.83     25000\n",
            "weighted avg       0.83      0.83      0.83     25000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10143  2331]\n",
            " [ 1884 10642]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using googlew2v_vectors\n",
        "X = np.array(googlew2v_vectors)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, imdb_df['sentiment'], test_size=0.5, shuffle=False)\n",
        "# defining RF classifier with chosen parameters\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,        # no of trees\n",
        "    max_depth=50,            # max depth of tree\n",
        "    min_samples_split=4,     # min samples required for split\n",
        "    min_samples_leaf=2,      # min samples required for leaf\n",
        "    random_state=0\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "# making predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "rfacc.append(accuracy)\n",
        "print(\"Best Model Accuracy:\", accuracy)\n",
        "# classification report\n",
        "cr = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\\n\", cr)\n",
        "# confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FQhyFcTNBlc",
        "outputId": "083cdb27-f6e5-4c23-9481-cbe8b8605f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model Accuracy: 0.80952\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.82      0.79      0.81     12474\n",
            "    positive       0.80      0.83      0.81     12526\n",
            "\n",
            "    accuracy                           0.81     25000\n",
            "   macro avg       0.81      0.81      0.81     25000\n",
            "weighted avg       0.81      0.81      0.81     25000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 9852  2622]\n",
            " [ 2140 10386]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rfacc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cQf49RTNShk",
        "outputId": "138741ff-2718-4fcd-bf87-d4e8f7eeb4e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.85076, 0.84772, 0.8314, 0.80952]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, with Random Forest Classifier, CountVectorizer provides the highest accuracy, followed closely by TfidfVectorizer."
      ],
      "metadata": {
        "id": "fMzYLO_YNkuN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the end, Word2Vec optimizes model training time by reducing vector size for input, but suffers from poorer accuracy in all three classifiers."
      ],
      "metadata": {
        "id": "9fmNqQEANvVI"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNKy00vPedyeN1LXC8Z5NBB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}